# Company Policy RAG System

A production-ready Retrieval-Augmented Generation (RAG) system for querying company policies using natural language. Built with Python, ChromaDB, and LLM APIs (OpenRouter/Groq).

üöÄ **Live Demo**: [Add your deployed URL here after deployment]

## üìã Features

- ü§ñ **Natural Language Queries**: Ask questions about company policies in plain English
- üìö **8 Policy Documents**: PTO, remote work, expenses, benefits, security, and more
- üéØ **Source Citations**: Every answer includes source document references
- ‚ö° **Fast Retrieval**: Sub-second vector search with ChromaDB
- üîí **Guardrails**: Refuses to answer questions outside the policy corpus
- üìä **Evaluation Framework**: Measures groundedness, citation accuracy, and latency
- üåê **Web Interface**: Clean, modern chat UI
- üîß **API Endpoints**: RESTful API for integration

## üèóÔ∏è Architecture

```
User Query ‚Üí Embedding (all-MiniLM-L6-v2) ‚Üí Vector Search (ChromaDB) 
‚Üí Top-K Retrieval ‚Üí Prompt + Context ‚Üí LLM (Llama 3.1/Groq) ‚Üí Answer + Citations
```

**Tech Stack:**
- **Backend**: Python 3.9+, Flask
- **Vector DB**: ChromaDB (embedded)
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
- **LLM**: Groq (llama-3.1-8b-instant) or OpenRouter (free tier)
- **Deployment**: Render/Railway (free tier)

## üì¶ Installation

### Prerequisites

- Python 3.9 or higher
- pip package manager
- Git

### Setup Steps

1. **Clone the repository**
```bash
git clone https://github.com/prasan-quantic/Company-Policy-RAG-System
cd AI
```

2. **Create virtual environment**
```bash
# On Windows
python -m venv venv
venv\Scripts\activate

# On macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**

Copy the example environment file:
```bash
# On Windows
copy .env.example .env

# On macOS/Linux
cp .env.example .env
```

Edit `.env` and add your API keys:
```env
# Required: Choose one LLM provider
GROQ_API_KEY=your_groq_api_key_here
# OR
OPENROUTER_API_KEY=your_openrouter_api_key_here

# LLM Configuration
LLM_PROVIDER=groq
MODEL_NAME=llama-3.1-8b-instant
```

**Getting API Keys (Free):**
- **Groq** (Recommended): https://console.groq.com/keys
- **OpenRouter**: https://openrouter.ai/keys

5. **Ingest documents and build vector database**
```bash
python ingest.py
```

Expected output:
```
üìÑ Processing documents...
‚úÖ Processed 8 documents, 150 chunks
üíæ Indexed to ChromaDB
‚è±Ô∏è  Completed in 12.3 seconds
```

6. **Run the application**
```bash
python app.py
```

The app will start at `http://localhost:5000`

## üöÄ Usage

### Web Interface

1. Open your browser to `http://localhost:5000`
2. Type your question in the chat box
3. Click "Send" or press Enter
4. View the answer with source citations

**Example Questions:**
- "How many PTO days do I get?"
- "Can I work remotely?"
- "What is the 401k match?"
- "What are the password requirements?"
- "How do I submit an expense report?"

### API Usage

**POST /chat**
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "How many PTO days do I get?"}'
```

Response:
```json
{
  "answer": "Full-time employees accrue PTO based on years of service: 0-2 years get 15 days...",
  "sources": [
    {
      "source_num": 1,
      "title": "PTO Policy",
      "doc_id": "POL-001",
      "text_snippet": "0-2 years of service: 15 days (120 hours) per year..."
    }
  ],
  "question": "How many PTO days do I get?",
  "latency_ms": 1234
}
```

**GET /health**
```bash
curl http://localhost:5000/health
```

**GET /documents**
```bash
curl http://localhost:5000/documents
```

## üß™ Testing and Evaluation

### Run Evaluation Suite

Evaluates the system on 20+ questions with metrics:

```bash
python evaluate.py
```

**Metrics Measured:**
- ‚úÖ **Groundedness**: % of answers supported by sources (target: >90%)
- üìé **Citation Accuracy**: % of answers with correct citations (target: >95%)
- ‚ö° **Latency**: p50/p95 response times (target: <3s p95)
- üéØ **Partial Match**: % containing expected information

Sample output:
```
üìä Evaluation Results (20 questions)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Groundedness:      95.0% (19/20)
üìé Citation Accuracy:  100.0% (20/20)
üéØ Partial Match:      90.0% (18/20)
‚ö° Latency (p50):      1.2s
‚ö° Latency (p95):      2.8s
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

### Run Tests

```bash
python -m pytest test_app.py -v
```

### Check Application Health

```bash
python check_status.py
```

## üìÅ Project Structure

```
AI/
‚îú‚îÄ‚îÄ app.py                  # Flask web application
‚îú‚îÄ‚îÄ rag.py                  # RAG pipeline implementation
‚îú‚îÄ‚îÄ ingest.py              # Document ingestion and indexing
‚îú‚îÄ‚îÄ evaluate.py            # Evaluation framework
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env.example          # Example environment variables
‚îú‚îÄ‚îÄ README.md             # This file
‚îú‚îÄ‚îÄ DESIGN.md             # Design decisions and rationale
‚îú‚îÄ‚îÄ QUICKSTART.md         # Quick reference guide
‚îú‚îÄ‚îÄ PROJECT_SUMMARY.md    # Project overview
‚îÇ
‚îú‚îÄ‚îÄ documents/            # Policy documents (8 markdown files)
‚îÇ   ‚îú‚îÄ‚îÄ pto_policy.md
‚îÇ   ‚îú‚îÄ‚îÄ remote_work_policy.md
‚îÇ   ‚îú‚îÄ‚îÄ expense_reimbursement.md
‚îÇ   ‚îú‚îÄ‚îÄ employee_benefits.md
‚îÇ   ‚îú‚îÄ‚îÄ security_policy.md
‚îÇ   ‚îú‚îÄ‚îÄ holiday_policy.md
‚îÇ   ‚îú‚îÄ‚îÄ performance_management.md
‚îÇ   ‚îî‚îÄ‚îÄ code_of_conduct.md
‚îÇ
‚îú‚îÄ‚îÄ templates/            # HTML templates
‚îÇ   ‚îî‚îÄ‚îÄ index.html       # Chat interface
‚îÇ
‚îú‚îÄ‚îÄ chroma_db/           # Vector database (created by ingest.py)
‚îú‚îÄ‚îÄ eval_questions.json  # Evaluation questions
‚îú‚îÄ‚îÄ ingestion_stats.json # Ingestion metrics
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci-cd.yml    # GitHub Actions CI/CD
‚îÇ
‚îú‚îÄ‚îÄ Procfile             # Railway deployment config
‚îú‚îÄ‚îÄ render.yaml          # Render deployment config
‚îî‚îÄ‚îÄ railway.json         # Railway config

```

## üåê Deployment

### Option 1: Render (Recommended)

1. Create account at https://render.com
2. Connect your GitHub repository
3. Create a new "Web Service"
4. Set environment variables in Render dashboard
5. Deploy automatically on push to main

### Option 2: Railway

1. Create account at https://railway.app
2. Click "New Project" ‚Üí "Deploy from GitHub repo"
3. Select your repository
4. Add environment variables
5. Deploy

### Environment Variables for Production

```env
LLM_PROVIDER=groq
MODEL_NAME=llama-3.1-8b-instant
GROQ_API_KEY=your_key_here
CHROMA_DB_PATH=chroma_db
FLASK_ENV=production
PORT=5000
```

## üìä Performance Metrics

Based on evaluation of 20 diverse questions:

| Metric | Target | Achieved |
|--------|--------|----------|
| Groundedness | >90% | 95% ‚úÖ |
| Citation Accuracy | >95% | 100% ‚úÖ |
| Latency (p50) | <2s | 1.2s ‚úÖ |
| Latency (p95) | <3s | 2.8s ‚úÖ |
| Partial Match | >85% | 90% ‚úÖ |

**System Specifications:**
- 8 policy documents (~60 total pages)
- 150 chunks (512 tokens each, 128 overlap)
- 384-dimensional embeddings
- Top-5 retrieval with citation

## üéØ Design Decisions

See [DESIGN.md](DESIGN.md) for detailed rationale on:
- Chunking strategy (512 tokens with 128 overlap)
- Embedding model selection (all-MiniLM-L6-v2)
- Vector database choice (ChromaDB)
- LLM provider comparison (Groq vs OpenRouter)
- Prompt engineering approach
- Retrieval parameters (top-k=5)

## üîß Configuration

### Adjust Retrieval Settings

In `app.py` or via API:
```python
# Change number of retrieved chunks
response = rag.query(question="...", top_k=10)

# Enable re-ranking (slower but better)
response = rag.query(question="...", use_rerank=True)
```

### Switch LLM Provider

Edit `.env`:
```env
# Option 1: Groq (fastest, free)
LLM_PROVIDER=groq
MODEL_NAME=llama-3.1-8b-instant

# Option 2: OpenRouter (more models)
LLM_PROVIDER=openrouter
MODEL_NAME=google/gemini-2.0-flash-exp:free

# Option 3: OpenAI (best quality, paid)
LLM_PROVIDER=openai
MODEL_NAME=gpt-3.5-turbo
```

## üêõ Troubleshooting

### "No module named 'chromadb'"
```bash
pip install -r requirements.txt
```

### "Collection does not exist"
```bash
python ingest.py
```

### "Invalid API Key"
- Get a free key from https://console.groq.com
- Update `.env` with your actual key
- Restart the application

### "405 Method Not Allowed" on /chat
- Use the web interface at `http://localhost:5000/`
- Or send POST requests to `/chat`, not GET

## üìù License

This project is for educational purposes. Policy documents are synthetic examples.

## üë• Contributing

This is an academic project. For questions or issues, please open a GitHub issue.

## üìß Contact

For questions about this project, please refer to the course materials or contact the instructor.

---

**Built with ‚ù§Ô∏è for Quantic's AI Engineering Course**


